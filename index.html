<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="Lifelike Agility and Play on Quadrupedal Robots">
  <meta property="og:title" content="Lifelike Agility and Play on Quadrupedal Robots"/>
  <meta property="og:description" content="Lifelike Agility and Play on Quadrupedal Robots"/>
  <meta property="og:url" content="https://tencent-roboticsx.github.io/lifelike-agility-and-play/"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/favicon.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="Lifelike Agility and Play on Quadrupedal Robots">
  <meta name="twitter:description" content="Lifelike Agility and Play on Quadrupedal Robots">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/favicon.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="Lifelike Agility and Play on Quadrupedal Robots, Reinforcement Learning, Generative Model, Pretraining">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Lifelike Agility and Play on Quadrupedal Robots</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Lifelike Agility and Play on Quadrupedal Robots using Reinforcement Learning and Generative Pre-trained Models</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <!-- <span class="author-block">
                <a href="FIRST AUTHOR PERSONAL LINK" target="_blank">First Author</a><sup>*</sup>,</span>
                <span class="author-block">
                  <a href="SECOND AUTHOR PERSONAL LINK" target="_blank">Second Author</a><sup>*</sup>,</span>
                  <span class="author-block">
                    <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Third Author</a>
                  </span> -->
                    <span class="author-block">
                        Lei Han<sup>*</sup>,
                    </span>
                    <span class="author-block">
                        Qingxu Zhu<sup>*</sup>,
                    </span>
                    <span class="author-block">
                        Jiapeng Sheng<sup>*</sup>,
                    </span>
                    <span class="author-block">
                        Chong Zhang<sup>*</sup>,
                    </span>
                    <span class="author-block">
                        Tingguang Li<sup>*</sup>,
                    </span>
                    <span class="author-block">
                        Yizheng Zhang<sup>*</sup>,
                    </span>
                    <span class="author-block">
                        He Zhang<sup>*</sup>,
                    </span>
                    <span class="author-block">
                        Yuzhen Liu,
                    </span>
                    <span class="author-block">
                        Cheng Zhou,
                    </span>
                    <span class="author-block">
                        Rui Zhao,
                    </span>
                    <span class="author-block">
                        Jie Li,
                    </span>
                    <span class="author-block">
                        Yufeng Zhang,
                    </span>
                    <span class="author-block">
                        Rui Wang,
                    </span>
                    <span class="author-block">
                        Wanchao Chi,
                    </span>
                    <span class="author-block">
                        Xiong Li,
                    </span>
                    <span class="author-block">
                        Yonghui Zhu,
                    </span>
                    <span class="author-block">
                        Lingzhu Xiang,
                    </span>
                    <span class="author-block">
                        Xiao Teng,
                    </span>
                    <span class="author-block">
                        Zhengyou Zhang
                    </span>

                  </div>

                  <div class="is-size-5">
                    <span class="eql-cntrb"><small><sup>*</sup>Equal Contribution</small></span>
                    <br><br>
                  </div>


                  <div class="is-size-3 publication-authors">
                    <span class="author-block">Tencent Robotics X<br>June, 2023</span>
                    <!-- <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span> -->
                  </div>

<!--                   <div class="is-size-5 publication-authors">
                    <span class="author-block">[Paper coming soon]</span>
                  </div> -->

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/abs/2308.15143" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- Supplementary PDF link -->
                    <!-- <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span> -->

                  <!-- Github link -->
                  <!-- <span class="link-block">
                    <a href="https://github.com/YOUR REPO HERE" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span> -->

                <!-- ArXiv abstract Link -->
                <!-- <span class="link-block">
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span> -->

            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<section class="hero teaser">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="publication-video">
            <!-- Youtube embed code here -->
            <iframe src="https://www.youtube.com/embed/ucucrLqT5dM" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
        <br>
      <h3 class="subtitle has-text-centered"><strong>Main Video for Lifelike Agility and Play on Quadrupedal Robots.</strong></h3>
    </div>
  </div>
</section>
<!-- End teaser video -->




<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            This project introduces the expressive power of recent advanced pretraining methods in computer vision and language understanding into motor control. Latent representations are pre-trained on animal motions. Then, the pre-trained representations are used to train control policies that can solve various challenging tasks through general reinforcement learning, demonstrating lifelike agility and versatile strategy. We consider many challenging tasks, including creeping, jumping over hurdles, freerunning over scattered blocks, etc. More interestingly, we design a multi-agent Chase Tag Game, in which two robots act as chaser and evader respectively, and their roles are switchable by triggering a Flag in the game. Real world experiments demonstrate that the MAX robots [1] deployed with the trained policy can emerge lifelike agility and strategy.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->




<!-- Paper Introduction -->
<section class="hero">
  <div class="hero-body">
    <div class="container">
      <div class="column has-text-centered">
        <h1 class="title">Pre-trained Representations by Imitating Animal Data on Flat Ground</h1>
          <div class="content has-text-justified">
            <div class="row">
            <div class="col-12">
              <br><p>
                 To achieve lifelike agility and strategy, we first collect a certain amount of regular animal motion data on flat ground using a motion capture system. Then, a neural controller containing a specific information bottleneck structure is trained to imitate the animal motions. 
                  The pre-trained latent representations are sufficiently expressive over the collected animal motions. In this learning stage, the network only takes the robot's proprioceptive information as input and does not perceive any information from the external environment.
                  For all real world experiments in this project, we deploy the trained controllers in simulation on the MAX robot via a zero-shot manner. MAX is a quadrupedal robot developed in-house. MAX is weighted 14 kg and each leg is with 3 actuators. Below, we demonstrate some imitation results on the MAX robots. 
              </p><br>
            </div>
            <div class="col-4">
                <img src="static/images/imitate1.gif"/>
                <!-- <h3 class="center">Imitation1</h3> -->
            </div>
            <div class="col-4">
                <img src="static/images/imitate2.gif"/>
                <!-- <h3 class="center">Imitation2</h3> -->
            </div>
            <div class="col-4">
                <img src="static/images/imitate3.gif"/>
                <!-- <h3 class="center">Imitation2 </h3> -->
            </div>

              <h3 class="center m-bottom">Performance of imitating animal motions after training latent representations over animal motion data.</h3>
          </div>
        </div>
        </div>
      </div>
    </div> 
  </div>
</section>
<!--End paper introduction -->





<!-- Paper Tasks Solving -->
<section class="section hero is-light">
  <div class="hero-body">
    <div class="container">
      <div class="column has-text-centered">
        <h1 class="title">Adapting to Environments</h1>
        <div class="content has-text-justified">
            <br><p>
                At a subsequent stage, we build additional neural network structures upon the pre-trained representations and let the network perceive exteroception from the environment. The aim at this stage is to align the latent representations compressing natural animal motions with the the environment, enabling the robot to act in response to the environment with animal-level behaviors. When the robot is capable to adapt to various complex environments, the knowledge that links animal motion representation with external perception will be stored in the neural network, again. Below, we demonstrate the learned performance on solving some challenging tasks that are not captured in the motion dataset.
             </p><br>
          

           <div class="columns is-centered has-text-centered">
              <div class="column is-20">
                <div class="normal-gif">
                  <!-- Youtube embed code here -->
                  <img src="static/images/creep.gif" style="max-width:1000px;width:60%" frameborder="0" allowfullscreen></img>
                  <h4 class="center">Creeping</h4>
                </div>
              </div>
            </div>
            <div class="columns is-centered has-text-centered">
              <div class="column is-20">
                <div class="normal-gif">
                  <!-- Youtube embed code here -->
                  <img src="static/images/stair.gif" style="max-width:1000px;width:60%" frameborder="0" allowfullscreen></img>
                  <h4 class="center">Ascending Stairs</h4>
                </div>
              </div>
            </div>
            <div class="columns is-centered has-text-centered">
              <div class="column is-20">
                <div class="normal-gif">
                  <!-- Youtube embed code here -->
                  <img src="static/images/hurdle.gif" style="max-width:1000px;width:60%" frameborder="0" allowfullscreen></img>
                  <h4 class="center">Jumping over Hurdles</h4>
                </div>
              </div>
            </div>
            <div class="columns is-centered has-text-centered">
              <div class="column is-20">
                <div class="normal-gif">
                  <!-- Youtube embed code here -->
                  <img src="static/images/block.gif" style="max-width:1000px;width:60%" frameborder="0" allowfullscreen></img>
                  <h4 class="center">Freerunning over Blocks</h4>
                </div>
              </div>
            </div>

          <h3 class="center m-bottom">Solving challenging tasks that are not captured in the motion dataset.</h3>


        </div>
      </div>
    </div> 
  </div>
</section>
<!--End Tasks Solving  -->




<!-- Paper ChaseTag Games -->
<section class="hero">
  <div class="hero-body">
    <div class="container">
      <div class="column has-text-centered">
        <h1 class="title">Chase Tag Game</h1>
        <div class="content has-text-justified">
          <div class="row">
            <div class="col-12">
              <!-- <h3 class="center m-bottom">Here is Chase Tag Games</h3> -->
              <p>
               Finally, we use the pre-trained networks obtained from the above two stages to solve a downstream task, a designed Chase Tag Game, to acquire knowledges at a strategic level. The knowledge trained at each stage can be expanded and adjusted without the need for re-training, allowing for continuous accumulation and learning.
                More interestingly, we design a multi-agent Chase Tag Game, in which two MAX robots play against each other to alternatively act the roles of chaser and evader, where the roles are determined by a flag scattered in the playground. The game setting is similar to the World Chase Tag [2], an international championship for competitive human parkour, with simplification and novel elements. Rules are introduced as below. The game is taken place in a 4.5m by 4.5m square ground with obstacles placed. At the beginning of each game episode, two MAX robots are randomly scattered in the play area, and an additional Flag is randomly placed as well. The roles of the robots are initially assigned randomly, where one MAX robot is the chaser and another is the evader. When the game starts, the mission of the chaser is to chase the evader, and as long as the distance between the two MAX robots is shorter than 0.6m, the game is terminated with the current chaser winning the game. Meanwhile, the evader aims to avoid being chased, while it has the chance to switch the roles of the current players that once it reaches the Flag within a distance of 0.3m; if this happens, the evader turns into the chaser while the previous chaser becomes the evader; and in the meantime, the Flag disappears and a new Flag will re-spawn randomly in the area. The game proceeds until the fact becomes true that the distance between the two robots is smaller than 0.6m. In all games, the average root forward velocity of the MAX robot is restricted to 0.5m/s. Several games can be watched below.
              </p>

            </div>
            <br><br><br>

            <div class="col-6">
                <img src="static/images/game1.gif"/>
                <h4 class="center">Round 1</h4>
            </div>
            <div class="col-6">
                <img src="static/images/game2.gif"/>
                <h4 class="center">Round 2</h4>
            </div>
            <div class="col-6">
                <img src="static/images/game3.gif"/>
                <h4 class="center">Round 3 </h4>
            </div>
            <div class="col-6">
                <img src="static/images/game4.gif"/>
                <h4 class="center">Round 4 </h4>
            </div>

            <div class="col-12">
              <!-- <h3 class="center m-bottom">Here is Chase Tag Games</h3> -->
              <p>
              Interestingly, we observe that the robots emerge lifelike strategies in the games. For example, the chaser will give up chasing if it estimates that there is no chance to catch the evader before it reaches the Flag in near future; at this time, the chaser will hesitate and wander around, waiting for the re-spawn of the new Flag. Moreover, at the very moment that the chaser is about tot catch the evader, the chaser prefers to perform a pounce like an animal pouncing on its prey; and similar behaviors can be observed when the evader is reaching the Flag. For more details, please refer to the video at the top of this webpage.
              </p>

            </div>
            <br><br><br>

          </div>
        </div>
      </div>
    </div> 
  </div>
</section>
<!--End paper poster -->




<!-- Movies -->
<section class="hero">
  <div class="hero-body">
    <div class="container">
      <div class="column has-text-centered">
        <h1 class="title">Movie 1</h1>
        <div class="publication-video">
          <!-- Youtube embed code here -->
            <iframe src="https://www.youtube.com/embed/x2A-bJ9d9bU" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
        <br>

        <h1 class="title">Movie 2</h1>
        <div class="publication-video">
          <!-- Youtube embed code here -->
            <iframe src="https://www.youtube.com/embed/m3qKQtcblH4" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
        <br>

        <h1 class="title">Movie 3</h1>
        <div class="publication-video">
          <!-- Youtube embed code here -->
            <iframe src="https://www.youtube.com/embed/kJwaBqGvmxY" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
        <br>

        <h1 class="title">Movie S1</h1>
        <div class="publication-video">
          <!-- Youtube embed code here -->
            <iframe src="https://www.youtube.com/embed/acMfARDynAc" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
        <br>

        <h1 class="title">Movie S2</h1>
        <div class="publication-video">
          <!-- Youtube embed code here -->
            <iframe src="https://www.youtube.com/embed/IcMQcRNsq4Q" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
        <br>

      </div>
    </div> 
  </div>
</section>
<!--End Movies -->









<!--BibTex citation -->
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-12">
        <div class="content">
          <strong>References:</strong>
            <br>
            <strong>[1]</strong> W. Chi, X. Jiang, and Y. Zheng, “A linearization of centroidal dynamics for the model-predictive control of quadruped robots,” in 2022 International Conference on Robotics and Automation (ICRA), 2022, pp. 4656–4663.
            <br>
            <strong>[2]</strong> <a href="https://en.wikipedia.org/wiki/World_Chase_Tag" target="_blank">https://en.wikipedia.org/wiki/World_Chase_Tag</a>.

            <br><br>

          <p>
            This page was built based on the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
